<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>nlp | 半夏琉璃空</title><meta name="author" content="听灵"><meta name="copyright" content="听灵"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="rnn 对于rnn的理解，我认为需看懂该图还有对应公式    隐藏层输出$$H_t &#x3D; \phi(X_t W_{xh} + H_t-1W_{hh} + b_h)$$  $H \in \mathbb{R}^{n \times h}$  $H$为隐藏层输出 $X_t \in \mathbb{R}^{n \times d}$ , $X_t$为小批量样本输入 $W_{xh} \in \mathb">
<meta property="og:type" content="article">
<meta property="og:title" content="nlp">
<meta property="og:url" content="http://example.com/2024/06/08/nlp/index.html">
<meta property="og:site_name" content="半夏琉璃空">
<meta property="og:description" content="rnn 对于rnn的理解，我认为需看懂该图还有对应公式    隐藏层输出$$H_t &#x3D; \phi(X_t W_{xh} + H_t-1W_{hh} + b_h)$$  $H \in \mathbb{R}^{n \times h}$  $H$为隐藏层输出 $X_t \in \mathbb{R}^{n \times d}$ , $X_t$为小批量样本输入 $W_{xh} \in \mathb">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/true.jpg">
<meta property="article:published_time" content="2024-06-08T12:01:35.000Z">
<meta property="article:modified_time" content="2024-09-26T11:58:31.499Z">
<meta property="article:author" content="听灵">
<meta property="article:tag" content="深度学习|自然语言处理">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/true.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2024/06/08/nlp/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        if (name && globalFn[key][name]) return
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'nlp',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-09-26 19:58:31'
}</script><meta name="generator" content="Hexo 7.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">半夏琉璃空</span></a><a class="nav-page-title" href="/"><span class="site-name">nlp</span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">nlp</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-06-08T12:01:35.000Z" title="发表于 2024-06-08 20:01:35">2024-06-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-26T11:58:31.499Z" title="更新于 2024-09-26 19:58:31">2024-09-26</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="rnn"><a href="#rnn" class="headerlink" title="rnn"></a>rnn</h1><ul>
<li>对于rnn的理解，我认为需看懂该图还有对应公式</li>
</ul>
<p><img src="/images/nlp/rnn.png"></p>
<ul>
<li><p>隐藏层输出<br>$$H_t &#x3D; \phi(X_t W_{xh} + H_t-1W_{hh} + b_h)$$</p>
<ul>
<li>$H \in \mathbb{R}^{n \times h}$  $H$为隐藏层输出</li>
<li>$X_t \in \mathbb{R}^{n \times d}$ , $X_t$为小批量样本输入</li>
<li>$W_{xh} \in \mathbb{R}^{d \times h}$  , $W_{xh}$隐藏层权重参数</li>
<li>$b_h \in \mathbb{R}^{1 \times h}$  , $b_h$为偏置参数</li>
</ul>
</li>
</ul>
<p><em>其中批量大小为n，输入维度为d，隐藏单元的数目为h</em></p>
<ul>
<li>输出层输出</li>
</ul>
<p>$$O &#x3D; HW_{hq} + b_q$$ </p>
<ul>
<li>$ O \in \mathbb{R}^{n \times q} $ , $O$为输出层输出</li>
<li>$ W_{hq} \in \mathbb{R}^{h \times q} $  ,  $W_{hq}为输出层的权重 </li>
<li>$ b_q \in \mathbb{R}^{1 \times q} $  , $b_q$为偏置参数</li>
</ul>
<p>值得一提的是，即使在不同的时间步，循环神经网络也总是使用这些模型参数。<br>因此，循环神经网络的参数开销不会随着时间步的增加而增加。</p>
<ul>
<li>自定义代码实现上述公式<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">rnn</span>(<span class="params">inputs, state, params</span>):</span><br><span class="line">    <span class="comment"># inputs的形状：(时间步数量，批量大小，词表大小)</span></span><br><span class="line">    W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="comment"># X的形状：(批量大小，词表大小)</span></span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)</span><br><span class="line">        Y = torch.mm(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H,)</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="gru"><a href="#gru" class="headerlink" title="gru"></a>gru</h1><ul>
<li>gru也是如此，从公式理解入手</li>
</ul>
<p><img src="/images/nlp/gru.png"></p>
<p>首先介绍<em>重置门</em>（reset gate）和<em>更新门</em>（update gate）。我们把它们设计成$(0, 1)$区间中的向量，这样我们就可以进行凸组合。</p>
<ul>
<li>重置门允许我们控制“可能还想记住”的过去状态的数量；</li>
<li>更新门将允许我们控制新状态中有多少个是旧状态的副本。</li>
</ul>
<p>数学表达</p>
<p>对于给定的时间步$t$，假设输入是一个小批量$\mathbf{X}<em>t \in \mathbb{R}^{n \times d}$（样本个数$n$，输入个数$d$），<br>上一个时间步的隐状态是$\mathbf{H}</em>{t-1} \in \mathbb{R}^{n \times h}$（隐藏单元个数$h$）。<br>那么，重置门$\mathbf{R}_t \in \mathbb{R}^{n \times h}$和更新门$\mathbf{Z}_t \in \mathbb{R}^{n \times h}$的计算如下所示：</p>
<p>$$<br>\begin{aligned}<br>\mathbf{R}<em>t &#x3D; \sigma(\mathbf{X}<em>t \mathbf{W}</em>{xr} + \mathbf{H}</em>{t-1} \mathbf{W}_{hr} + \mathbf{b}<em>r),\<br>\mathbf{Z}<em>t &#x3D; \sigma(\mathbf{X}<em>t \mathbf{W}</em>{xz} + \mathbf{H}</em>{t-1} \mathbf{W}</em>{hz} + \mathbf{b}_z),<br>\end{aligned}<br>$$</p>
<p>其中</p>
<ul>
<li>$\mathbf{W}<em>{xr}, \mathbf{W}</em>{xz} \in \mathbb{R}^{d \times h}$</li>
<li>$\mathbf{W}<em>{hr}, \mathbf{W}</em>{hz} \in \mathbb{R}^{h \times h}$</li>
<li>$\mathbf{b}_r, \mathbf{b}_z \in \mathbb{R}^{1 \times h}$是偏置参数。</li>
</ul>
<p>将输入值转换到区间$(0, 1)$。</p>
<h2 id="候选隐状态"><a href="#候选隐状态" class="headerlink" title="候选隐状态"></a>候选隐状态</h2><p>时间步$t$的<em>候选隐状态</em>（candidate hidden state）$\tilde{\mathbf{H}}_t \in \mathbb{R}^{n \times h}$。<br>$$\tilde{\mathbf{H}}<em>t &#x3D; \tanh(\mathbf{X}<em>t \mathbf{W}</em>{xh} + \left(\mathbf{R}<em>t \odot \mathbf{H}</em>{t-1}\right) \mathbf{W}</em>{hh} + \mathbf{b}_h),$$</p>
<p>其中</p>
<ul>
<li>$\mathbf{W}_{xh} \in \mathbb{R}^{d \times h}$</li>
<li>$\mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$是权重参数</li>
<li>$\mathbf{b}_h \in \mathbb{R}^{1 \times h}$是偏置项</li>
<li>符号$\odot$是Hadamard积（按元素乘积）运算符。</li>
</ul>
<p>在这里，我们使用tanh非线性激活函数来确保候选隐状态中的值保持在区间$(-1, 1)$中。</p>
<p><strong>从极端假设情况入手较好理解</strong></p>
<ul>
<li>每当重置门$\mathbf{R}_t$中的项接近$1$时，和rnn的隐藏层输出一致</li>
<li>对于重置门$\mathbf{R}_t$中所有接近$0$的项，候选隐状态是以$\mathbf{X}_t$作为输入的多层感知机的结果。预先存在的隐状态被<strong>重置</strong></li>
</ul>
<h2 id="隐状态"><a href="#隐状态" class="headerlink" title="隐状态"></a>隐状态</h2><p>$$\mathbf{H}_t &#x3D; \mathbf{Z}<em>t \odot \mathbf{H}</em>{t-1}  + (1 - \mathbf{Z}_t) \odot \tilde{\mathbf{H}}_t.$$</p>
<ul>
<li>每当更新门$\mathbf{Z}_t$接近$1$时，模型就倾向只保留旧状态。此时，来自$\mathbf{X}_t$的信息基本上被忽略，从而有效地跳过了依赖链条中的时间步$t$。</li>
<li>相反，当$\mathbf{Z}_t$接近$0$时，新的隐状态$\mathbf{H}_t$就会接近候选隐状态$\tilde{\mathbf{H}}_t$。</li>
</ul>
<p>这些设计可以帮助我们处理循环神经网络中的梯度消失问题，并更好地捕获时间步距离很长的序列的依赖关系。</p>
<p><em>例如，如果整个子序列的所有时间步的更新门都接近于$1$，则无论序列的长度如何，在序列起始时间步的旧隐状态都将很容易保留并传递到序列结束。</em></p>
<ul>
<li>同样贴上该网络关键代码实现以上步骤，加强理解<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gru</span>(<span class="params">inputs, state, params</span>):</span><br><span class="line">    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)</span><br><span class="line">        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)</span><br><span class="line">        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)</span><br><span class="line">        H = Z * H + (<span class="number">1</span> - Z) * H_tilda</span><br><span class="line">        Y = H @ W_hq + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H,)</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="lstm"><a href="#lstm" class="headerlink" title="lstm"></a>lstm</h1><ul>
<li>lstm的参数设计有些晦涩难懂，但其实理解公式就不影响对其的使用</li>
</ul>
<p><img src="/images/nlp/lstm.png">)</p>
<p>数学表达</p>
<p>假设有$h$个隐藏单元，批量大小为$n$，输入数为$d$。因此，输入为$\mathbf{X}<em>t \in \mathbb{R}^{n \times d}$，前一时间步的隐状态为$\mathbf{H}</em>{t-1} \in \mathbb{R}^{n \times h}$。相应地，时间步$t$的门被定义如下：</p>
<ul>
<li>输入门是$\mathbf{I}_t \in \mathbb{R}^{n \times h}$，</li>
<li>遗忘门是$\mathbf{F}_t \in \mathbb{R}^{n \times h}$，</li>
<li>输出门是$\mathbf{O}_t \in \mathbb{R}^{n \times h}$。</li>
</ul>
<p>它们的计算方法如下：</p>
<p>$$<br>\begin{aligned}<br>\mathbf{I}<em>t &amp;&#x3D; \sigma(\mathbf{X}<em>t \mathbf{W}</em>{xi} + \mathbf{H}</em>{t-1} \mathbf{W}_{hi} + \mathbf{b}<em>i),\<br>\mathbf{F}<em>t &amp;&#x3D; \sigma(\mathbf{X}<em>t \mathbf{W}</em>{xf} + \mathbf{H}</em>{t-1} \mathbf{W}</em>{hf} + \mathbf{b}<em>f),\<br>\mathbf{O}<em>t &amp;&#x3D; \sigma(\mathbf{X}<em>t \mathbf{W}</em>{xo} + \mathbf{H}</em>{t-1} \mathbf{W}</em>{ho} + \mathbf{b}_o),<br>\end{aligned}<br>$$</p>
<p>其中</p>
<ul>
<li>$\mathbf{W}<em>{xi}, \mathbf{W}</em>{xf}, \mathbf{W}_{xo} \in \mathbb{R}^{d \times h}$</li>
<li>$\mathbf{W}<em>{hi}, \mathbf{W}</em>{hf}, \mathbf{W}_{ho} \in \mathbb{R}^{h \times h}$，</li>
<li>$\mathbf{b}_i, \mathbf{b}_f, \mathbf{b}_o \in \mathbb{R}^{1 \times h}$</li>
</ul>
<h2 id="候选记忆元"><a href="#候选记忆元" class="headerlink" title="候选记忆元"></a>候选记忆元</h2><p>$\tilde{\mathbf{C}}_t \in \mathbb{R}^{n \times h}$。它的计算与上面描述的三个门的计算类似，但是使用$\tanh$函数作为激活函数，函数的值范围为$(-1, 1)$。下面导出在时间步$t$处的方程：</p>
<p>$$\tilde{\mathbf{C}}<em>t &#x3D; \text{tanh}(\mathbf{X}<em>t \mathbf{W}</em>{xc} + \mathbf{H}</em>{t-1} \mathbf{W}_{hc} + \mathbf{b}_c),$$</p>
<p>其中</p>
<ul>
<li>$\mathbf{W}_{xc} \in \mathbb{R}^{d \times h}$和</li>
<li>$\mathbf{W}_{hc} \in \mathbb{R}^{h \times h}$是权重参数，</li>
<li>$\mathbf{b}_c \in \mathbb{R}^{1 \times h}$是偏置参数。</li>
</ul>
<h2 id="记忆元"><a href="#记忆元" class="headerlink" title="记忆元"></a>记忆元</h2><p>输入门$\mathbf{I}_t$控制采用多少来自$\tilde{\mathbf{C}}_t$的新数据，而遗忘门$\mathbf{F}<em>t$控制保留多少过去的记忆元$\mathbf{C}</em>{t-1} \in \mathbb{R}^{n \times h}$的内容。使用按元素乘法，得出：</p>
<p>$$\mathbf{C}_t &#x3D; \mathbf{F}<em>t \odot \mathbf{C}</em>{t-1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t.$$</p>
<ul>
<li>如果遗忘门始终为$1$且输入门始终为$0$，则过去的记忆元$\mathbf{C}_{t-1}$将随时间被保存并传递到当前时间步。<br><em>引入这种设计是为了缓解梯度消失问题，并更好地捕获序列中的长距离依赖关系。</em></li>
</ul>
<h2 id="隐状态-1"><a href="#隐状态-1" class="headerlink" title="隐状态"></a>隐状态</h2><p>最后，我们需要定义如何计算隐状态$\mathbf{H}_t \in \mathbb{R}^{n \times h}$，这就是输出门发挥作用的地方。<br>在长短期记忆网络中，它仅仅是记忆元的$\tanh$的门控版本。<br>这就确保了$\mathbf{H}_t$的值始终在区间$(-1, 1)$内：</p>
<p>$$\mathbf{H}_t &#x3D; \mathbf{O}_t \odot \tanh(\mathbf{C}_t).$$</p>
<ul>
<li><p>只要输出门接近$1$，我们就能够有效地将所有记忆信息传递给预测部分，</p>
</li>
<li><p>而对于输出门接近$0$，我们只保留记忆元内的所有信息，而不需要更新隐状态。</p>
</li>
<li><p>贴上关键代码有助理解</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lstm</span>(<span class="params">inputs, state, params</span>):</span><br><span class="line">    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,</span><br><span class="line">     W_hq, b_q] = params</span><br><span class="line">    (H, C) = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)</span><br><span class="line">        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)</span><br><span class="line">        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)</span><br><span class="line">        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)</span><br><span class="line">        C = F * C + I * C_tilda</span><br><span class="line">        H = O * torch.tanh(C)</span><br><span class="line">        Y = (H @ W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H, C)</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="deep-rnn"><a href="#deep-rnn" class="headerlink" title="deep_rnn"></a>deep_rnn</h1><p>我们可以将多层循环神经网络堆叠在一起，通过对几个简单层的组合，产生了一个灵活的机制。特别是，数据可能与不同层的堆叠有关。</p>
<p>每个隐状态都连续地传递到当前层的下一个时间步和下一层的当前时间步。</p>
<p><img src="/images/nlp/deep_rnn.png"></p>
<h2 id="函数依赖关系"><a href="#函数依赖关系" class="headerlink" title="函数依赖关系"></a>函数依赖关系</h2><p>我们可以将深度架构中的函数依赖关系形式化，</p>
<p>假设在时间步$t$有一个小批量的输入数据$\mathbf{X}_t \in \mathbb{R}^{n \times d}$（样本数：$n$，每个样本中的输入数：$d$）。<br>同时，将$l^\mathrm{th}$隐藏层（$l&#x3D;1,\ldots,L$）的隐状态设为$\mathbf{H}_t^{(l)}  \in \mathbb{R}^{n \times h}$（隐藏单元数：$h$），<br>输出层变量设为$\mathbf{O}_t \in \mathbb{R}^{n \times q}$（输出数：$q$）。<br>设置$\mathbf{H}_t^{(0)} &#x3D; \mathbf{X}_t$，第$l$个隐藏层的隐状态使用激活函数$\phi_l$，则：</p>
<p>$$\mathbf{H}<em>t^{(l)} &#x3D; \phi_l(\mathbf{H}<em>t^{(l-1)} \mathbf{W}</em>{xh}^{(l)} + \mathbf{H}</em>{t-1}^{(l)} \mathbf{W}_{hh}^{(l)}  + \mathbf{b}_h^{(l)}),$$</p>
<p>其中</p>
<ul>
<li>$\mathbf{W}_{xh}^{(l)} \in \mathbb{R}^{h \times h}$，</li>
<li>$\mathbf{W}_{hh}^{(l)} \in \mathbb{R}^{h \times h}$</li>
<li>$\mathbf{b}_h^{(l)} \in \mathbb{R}^{1 \times h}$</li>
</ul>
<p>最后，输出层的计算仅基于第$l$个隐藏层最终的隐状态：</p>
<p>$$\mathbf{O}_t &#x3D; \mathbf{H}<em>t^{(L)} \mathbf{W}</em>{hq} + \mathbf{b}_q,$$</p>
<p>其中</p>
<ul>
<li>$\mathbf{W}_{hq} \in \mathbb{R}^{h \times q}$</li>
<li>$\mathbf{b}_q \in \mathbb{R}^{1 \times q}$都是输出层的模型参数。</li>
</ul>
<p>与多层感知机一样，隐藏层数目$L$和隐藏单元数目$h$都是超参数。</p>
<h1 id="bi-rnn"><a href="#bi-rnn" class="headerlink" title="bi_rnn"></a>bi_rnn</h1><p><strong>实现</strong></p>
<p>增加一个“从最后一个词元开始从后向前运行”的循环神经网络，而不是只有一个在前向模式下“从第一个词元开始运行”的循环神经网络。<em>双向循环神经网络</em>（bidirectional RNNs）</p>
<p><img src="/images/nlp/bi_rnn.png"></p>
<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>对于任意时间步$t$，给定一个小批量的输入数据$\mathbf{X}_t \in \mathbb{R}^{n \times d}$（样本数$n$，每个示例中的输入数$d$），并且令隐藏层激活函数为$\phi$。<br>在双向架构中，我们设该时间步的前向和反向隐状态分别为$\overrightarrow{\mathbf{H}}_t  \in \mathbb{R}^{n \times h}$和$\overleftarrow{\mathbf{H}}_t  \in \mathbb{R}^{n \times h}$，<br>其中$h$是隐藏单元的数目。前向和反向隐状态的更新如下：</p>
<p>$$<br>\begin{aligned}<br>\overrightarrow{\mathbf{H}}<em>t &amp;&#x3D; \phi(\mathbf{X}<em>t \mathbf{W}</em>{xh}^{(f)} + \overrightarrow{\mathbf{H}}</em>{t-1} \mathbf{W}_{hh}^{(f)}  + \mathbf{b}<em>h^{(f)}),\<br>\overleftarrow{\mathbf{H}}<em>t &amp;&#x3D; \phi(\mathbf{X}<em>t \mathbf{W}</em>{xh}^{(b)} + \overleftarrow{\mathbf{H}}</em>{t+1} \mathbf{W}</em>{hh}^{(b)}  + \mathbf{b}_h^{(b)}),<br>\end{aligned}<br>$$</p>
<p>其中</p>
<ul>
<li>$\mathbf{W}<em>{xh}^{(f)} \in \mathbb{R}^{d \times h}, \mathbf{W}</em>{hh}^{(f)} \in \mathbb{R}^{h \times h}, \mathbf{W}<em>{xh}^{(b)} \in \mathbb{R}^{d \times h}, \mathbf{W}</em>{hh}^{(b)} \in \mathbb{R}^{h \times h}$</li>
<li>$\mathbf{b}_h^{(f)} \in \mathbb{R}^{1 \times h}, \mathbf{b}_h^{(b)} \in \mathbb{R}^{1 \times h}$</li>
</ul>
<p>接下来，将前向隐状态$\overrightarrow{\mathbf{H}}_t$和反向隐状态$\overleftarrow{\mathbf{H}}_t$连接起来，获得需要送入输出层的隐状态$\mathbf{H}_t \in \mathbb{R}^{n \times 2h}$。<br>在具有多个隐藏层的深度双向循环神经网络中，该信息作为输入传递到下一个双向层。<br>最后，输出层计算得到的输出为$\mathbf{O}_t \in \mathbb{R}^{n \times q}$（$q$是输出单元的数目）：</p>
<p>$$\mathbf{O}_t &#x3D; \mathbf{H}<em>t \mathbf{W}</em>{hq} + \mathbf{b}_q.$$</p>
<p>这里，权重矩阵$\mathbf{W}_{hq} \in \mathbb{R}^{2h \times q}$和偏置$\mathbf{b}_q \in \mathbb{R}^{1 \times q}$是输出层的模型参数。<br>事实上，这两个方向可以拥有不同数量的隐藏单元。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">听灵</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/06/08/nlp/">http://example.com/2024/06/08/nlp/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">半夏琉璃空</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">深度学习|自然语言处理</a></div><div class="post-share"><div class="social-share" data-image="/images/true.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="prev-post pull-left" href="/2024/06/13/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/" title="计算机网络期末复习"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">计算机网络期末复习</div></div></a><a class="next-post pull-right" href="/2024/06/06/clip/" title="clip"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">clip</div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info is-center"><div class="avatar-img"><img src="/images/true.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">听灵</div><div class="author-info-description">Journey of a thousand miles begins with single step</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">14</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">9</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#rnn"><span class="toc-number">1.</span> <span class="toc-text">rnn</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#gru"><span class="toc-number">2.</span> <span class="toc-text">gru</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%80%99%E9%80%89%E9%9A%90%E7%8A%B6%E6%80%81"><span class="toc-number">2.1.</span> <span class="toc-text">候选隐状态</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%90%E7%8A%B6%E6%80%81"><span class="toc-number">2.2.</span> <span class="toc-text">隐状态</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#lstm"><span class="toc-number">3.</span> <span class="toc-text">lstm</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%80%99%E9%80%89%E8%AE%B0%E5%BF%86%E5%85%83"><span class="toc-number">3.1.</span> <span class="toc-text">候选记忆元</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%B0%E5%BF%86%E5%85%83"><span class="toc-number">3.2.</span> <span class="toc-text">记忆元</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%90%E7%8A%B6%E6%80%81-1"><span class="toc-number">3.3.</span> <span class="toc-text">隐状态</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#deep-rnn"><span class="toc-number">4.</span> <span class="toc-text">deep_rnn</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB"><span class="toc-number">4.1.</span> <span class="toc-text">函数依赖关系</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#bi-rnn"><span class="toc-number">5.</span> <span class="toc-text">bi_rnn</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-number">5.1.</span> <span class="toc-text">定义</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/27/%E7%A7%91%E7%A0%94%E5%91%A8%E6%8A%A52/" title="科研周报week_2">科研周报week_2</a><time datetime="2024-09-27T14:41:40.000Z" title="发表于 2024-09-27 22:41:40">2024-09-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/25/CIL/" title="分析类增量学习">分析类增量学习</a><time datetime="2024-09-25T08:07:03.000Z" title="发表于 2024-09-25 16:07:03">2024-09-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/24/%E8%BE%B9%E7%BC%98%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0/" title="边缘持续学习">边缘持续学习</a><time datetime="2024-09-24T06:55:17.000Z" title="发表于 2024-09-24 14:55:17">2024-09-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/08/12/datawhale/" title="datawhale夏令营(2)">datawhale夏令营(2)</a><time datetime="2024-08-12T09:09:36.000Z" title="发表于 2024-08-12 17:09:36">2024-08-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/08/11/AIGC%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/" title="AIGC快速入门">AIGC快速入门</a><time datetime="2024-08-11T13:36:20.000Z" title="发表于 2024-08-11 21:36:20">2024-08-11</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By 听灵</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>